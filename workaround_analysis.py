#!/usr/bin/env python3
"""
å˜é€šæ–¹æ¡ˆï¼šä½¿ç”¨å½“å‰æ•°æ®åˆ†æV2Xé€šä¿¡ï¼ˆå¿½ç•¥vehicle_idé—®é¢˜ï¼‰

ç”±äºå½“å‰æ•°æ®çš„vehicle_idè¯†åˆ«ä¸å®Œæ•´ï¼Œè¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•
åœ¨ä¸ä¾èµ–vehicle_idçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†æã€‚
"""

import pandas as pd
from pathlib import Path

BASE_PATH = Path("output/processed_full")

print("="*70)
print("V2Xæ•°æ®åˆ†æ - å˜é€šæ–¹æ¡ˆï¼ˆå¿½ç•¥vehicle_idé—®é¢˜ï¼‰")
print("="*70)

# ============================================================================
# æ–¹æ¡ˆ1: åªåˆ†ææœ‰æ•ˆçš„V2Xæ¶ˆæ¯ï¼ˆæ’é™¤unknownï¼‰
# ============================================================================

print("\n[æ–¹æ¡ˆ1] åˆ†ææœ‰station_idçš„V2Xæ¶ˆæ¯...")

v2x = pd.read_parquet(BASE_PATH / "v2x_messages.parquet")
v2x['timestamp'] = pd.to_datetime(v2x['timestamp_ms'], unit='ms')

# è¿‡æ»¤å‡ºæœ‰æ•ˆçš„è½¦è¾†ID
valid_v2x = v2x[v2x['vehicle_id'] != 'unknown'].copy()

print(f"  åŸå§‹V2Xæ¶ˆæ¯: {len(v2x):,}")
print(f"  æœ‰æ•ˆæ¶ˆæ¯: {len(valid_v2x):,} ({len(valid_v2x)/len(v2x)*100:.1f}%)")
print(f"  å®é™…è½¦è¾†æ•°: {valid_v2x['vehicle_id'].nunique()}")

# åˆ†ææ¯ä¸ªè½¦è¾†çš„é€šä¿¡è¡Œä¸º
vehicle_stats = valid_v2x.groupby('vehicle_id').agg({
    'message_size_bytes': ['count', 'sum', 'mean'],
    'timestamp_ms': ['min', 'max']
}).reset_index()

vehicle_stats.columns = ['vehicle_id', 'msg_count', 'total_bytes', 'avg_size', 'first_seen', 'last_seen']
vehicle_stats['duration_hours'] = (vehicle_stats['last_seen'] - vehicle_stats['first_seen']) / (1000 * 3600)

print(f"\nè½¦è¾†é€šä¿¡ç»Ÿè®¡ï¼ˆæœ‰æ•ˆè½¦è¾†ï¼‰:")
print(vehicle_stats[['msg_count', 'total_bytes', 'duration_hours']].describe())

# Top 10æ´»è·ƒè½¦è¾†
print(f"\næœ€æ´»è·ƒçš„10è¾†è½¦:")
top10 = vehicle_stats.nlargest(10, 'msg_count')[['vehicle_id', 'msg_count', 'total_bytes', 'duration_hours']]
print(top10.to_string())

# ============================================================================
# æ–¹æ¡ˆ2: ç©ºé—´èšç±»åˆ†æï¼ˆä¸ä¾èµ–vehicle_idï¼‰
# ============================================================================

print("\n\n[æ–¹æ¡ˆ2] åŸºäºä½ç½®çš„èšç±»åˆ†æï¼ˆä¸ä½¿ç”¨vehicle_idï¼‰...")

fused = pd.read_parquet(BASE_PATH / "fused_data.parquet")
fused['timestamp'] = pd.to_datetime(fused['timestamp_ms'], unit='ms')

# åˆ›å»ºç©ºé—´ç½‘æ ¼
grid_size = 0.001  # çº¦110ç±³
fused['lat_grid'] = (fused['latitude'] / grid_size).astype(int)
fused['lon_grid'] = (fused['longitude'] / grid_size).astype(int)

# ç»Ÿè®¡æ¯ä¸ªç½‘æ ¼çš„æ´»åŠ¨
grid_stats = fused.groupby(['lat_grid', 'lon_grid']).agg({
    'messages_sent': 'sum',
    'total_bytes_sent': 'sum',
    'timestamp_ms': 'count'
}).reset_index()

grid_stats.columns = ['lat_grid', 'lon_grid', 'total_messages', 'total_bytes', 'num_samples']

# åªçœ‹æœ‰é€šä¿¡æ´»åŠ¨çš„ç½‘æ ¼
active_grids = grid_stats[grid_stats['total_messages'] > 0]

print(f"  æ€»ç½‘æ ¼æ•°: {len(grid_stats):,}")
print(f"  æœ‰é€šä¿¡æ´»åŠ¨çš„ç½‘æ ¼: {len(active_grids):,}")
print(f"  å¹³å‡æ¯ç½‘æ ¼æ¶ˆæ¯æ•°: {active_grids['total_messages'].mean():.1f}")

# çƒ­ç‚¹åŒºåŸŸ
print(f"\né€šä¿¡çƒ­ç‚¹åŒºåŸŸï¼ˆTop 10ï¼‰:")
hotspots = active_grids.nlargest(10, 'total_messages')
print(hotspots.to_string())

# ============================================================================
# æ–¹æ¡ˆ3: æ—¶é—´æ¨¡å¼åˆ†æï¼ˆä¸ä¾èµ–vehicle_idï¼‰
# ============================================================================

print("\n\n[æ–¹æ¡ˆ3] æ—¶é—´æ¨¡å¼åˆ†æ...")

v2x['hour'] = v2x['timestamp'].dt.hour
v2x['date'] = v2x['timestamp'].dt.date

# æŒ‰å°æ—¶ç»Ÿè®¡
hourly = v2x.groupby('hour').agg({
    'message_size_bytes': ['count', 'sum']
}).reset_index()
hourly.columns = ['hour', 'msg_count', 'total_bytes']

print(f"\næ¯å°æ—¶æ¶ˆæ¯åˆ†å¸ƒ:")
print(hourly.to_string())

# æŒ‰æ—¥æœŸç»Ÿè®¡
daily = v2x.groupby('date').agg({
    'message_size_bytes': ['count', 'sum']
}).reset_index()
daily.columns = ['date', 'msg_count', 'total_bytes']

print(f"\næ¯æ—¥æ¶ˆæ¯é‡:")
print(daily.head(10).to_string())

# ============================================================================
# æ–¹æ¡ˆ4: æ¶ˆæ¯ç±»å‹åˆ†æ
# ============================================================================

print("\n\n[æ–¹æ¡ˆ4] æ¶ˆæ¯ç±»å‹åˆ†æ...")

msg_type_stats = v2x.groupby('message_type').agg({
    'message_size_bytes': ['count', 'mean', 'std', 'min', 'max']
}).reset_index()

print(f"\næ¶ˆæ¯ç±»å‹ç»Ÿè®¡:")
print(msg_type_stats.to_string())

# ============================================================================
# æ–¹æ¡ˆ5: å¯¼å‡ºæœ‰æ•ˆæ•°æ®ç”¨äºåç»­åˆ†æ
# ============================================================================

print("\n\n[æ–¹æ¡ˆ5] å¯¼å‡ºæœ‰æ•ˆæ•°æ®...")

# åªä¿å­˜æœ‰vehicle_idçš„æ•°æ®
valid_v2x.to_parquet(BASE_PATH / "v2x_messages_valid.parquet", index=False)
print(f"  âœ“ å¯¼å‡ºæœ‰æ•ˆV2Xæ¶ˆæ¯åˆ°: {BASE_PATH / 'v2x_messages_valid.parquet'}")
print(f"    ({len(valid_v2x):,} æ¡è®°å½•, {valid_v2x['vehicle_id'].nunique()} è¾†è½¦)")

# å¯¼å‡ºæ´»è·ƒç½‘æ ¼æ•°æ®
active_grids.to_csv(BASE_PATH / "spatial_hotspots.csv", index=False)
print(f"  âœ“ å¯¼å‡ºç©ºé—´çƒ­ç‚¹æ•°æ®åˆ°: {BASE_PATH / 'spatial_hotspots.csv'}")

# å¯¼å‡ºè½¦è¾†ç»Ÿè®¡
vehicle_stats.to_csv(BASE_PATH / "vehicle_statistics.csv", index=False)
print(f"  âœ“ å¯¼å‡ºè½¦è¾†ç»Ÿè®¡åˆ°: {BASE_PATH / 'vehicle_statistics.csv'}")

print("\n" + "="*70)
print("åˆ†æå®Œæˆï¼")
print("="*70)

print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
print("  1. ä½¿ç”¨ v2x_messages_valid.parquet è¿›è¡Œè½¦è¾†çº§åˆ«çš„åˆ†æ")
print("  2. ä½¿ç”¨ spatial_hotspots.csv è¿›è¡Œç©ºé—´åˆ†æ")
print("  3. ä½¿ç”¨ vehicle_statistics.csv äº†è§£å„è½¦è¾†çš„é€šä¿¡è¡Œä¸º")
print("  4. trajectories å’Œ fused_data å¯ç”¨äºæ•´ä½“æ—¶ç©ºåˆ†æï¼ˆä¸åŒºåˆ†è½¦è¾†ï¼‰")

print("\nâš ï¸  å±€é™æ€§:")
print("  - æ— æ³•è¿½è¸ªå•ä¸ªè½¦è¾†çš„å®Œæ•´è½¨è¿¹")
print("  - æ— æ³•åšè½¦è¾†é—´çš„äº¤äº’åˆ†æï¼ˆV2Vï¼‰")
print("  - å»ºè®®æŒ‰ VEHICLE_ID_ISSUE.md ä¸­çš„æ–¹æ¡ˆæ”¹è¿›å¤„ç†ä»£ç åé‡æ–°å¤„ç†")
